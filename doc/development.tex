\chapter{Development}
\section{Application Architecture}
\label{sec:architecture}
All the iOS applications follow the model view controller architecture. This
architecture separates the data model ---inside the \emph{model}---, the presentation
of the data ---the \emph{view}--- and the interaction and logic between them ---the
\emph{controller}---. 

First of all we're going to discuss how Ponster applies the MVC
architecture; then we will introduce the selected persistence layer with CoreData
and finally how the augmented reality fits into the app.

\subsection{Model View Controller}
Each of the main components of Ponster is represented by a subclass of UIKit's
controller, the \texttt{UIViewController}. When developing complex applications is
frequent to have a base view controller with shared functionality. Then, the rest of
the view controllers inherit from it. In Ponster, the main view controller from
where all of our controllers inherit from are the ones presented by UIKit, without
any other feature added. 

We can separate the view controllers in our app with the following list:
\begin{itemize}
\item Main screen view controller.
\begin{itemize}
\item Collection view controller.
\end{itemize}
\item Poster view controller
\item Augmented reality view controller.
\end{itemize}

There is one view controller that is built from two view controllers, the main
screen. It is common in iOS to represent tables or collection views using the UIKit
view controller that is ready for those tasks, \texttt{UITableViewController} or
\texttt{UICollectionViewController}. Usually this is done because both view
controllers have built-in methods like the refresh control that are easier and
more correct to use when sub-classing from those UIKit view controllers. In order to
customize the rest of the view controller and to keep responsibility separated ---one
view manages the collection, the other the whole screen--- we use
view controller containment to embed the collection VC inside the main screen view
controller. This allows us to keep the single responsibility principle and to have
lighter view controllers.

Apart of the view controllers, all the data model is separated into
\texttt{NSManaged\-Object} subclasses. Each of that subclass represent an entity in
our model. Business logic is usually implemented as a category of each of the model
subclass. Categories in Objective-C are capable of adding methods to any object
without having to change its implementations. This logic is usually added as a
category in order to not interfere with the \texttt{NSManagedObject} subclass,
because every time a change is made to a model, we have to generate the subclass
again and this would erase all the logic that we could have built inside.

The only custom view subclass we have in Ponster is the cell subclass to
represent each poster in the main screen view controller. The
\texttt{UI\-Collection\-View\-Cell} subclass called \texttt{PNSPoster\-Collection\-View\-Cell}
encapsulates all the views that are needed to display the poster image and it's
title. We just need to provide the \texttt{Poster} entity and this subclass manages
to draw the entire view.

\subsection{Persistence layer architecture}
\label{sec:persistency}
In order to deliver a good user experience, we have to understand the iOS
architecture. iOS has a high-priority thread called \emph{Main Thread} where all the
UIKit operations must be executed. Thanks to this, the responsiveness of every UI
has the top execution priority, but it also has downsides. If our code is blocking
the Main Thread, it will also block the user interface, thus delivering a poor user
experience. To solve this potential issue, we have to send all the possible
operations to another threads.

When using a CoreData model, it is a good idea to create different
\texttt{NSManaged\-Object\-Context}s in order to have contexts using background threads
and one context tied to the UI code to provide the views with the database
objects. If we send all the saves to the background queue, the risk of blocking the
Main Thread is reduced. Our proposed CoreData architecture~\cite{coredataarch} has
one \emph{background} context attached to the persistent store, another one attached
to the Main Thread to use it when providing data to our views and several background
contexts to perform saves.

\begin{figure}
\centering
\includegraphics[scale=0.75]{img/coredataarch.png}
\caption{\label{fig:coredataarch}CoreData context architecture. Taken
  from~\cite{coredataarch}} 
\end{figure} 

The context attached to the persistent store saves all the data on disk, but on a
background thread, so the UI is never blocked. Then, all the save contexts that we
can create use a background thread and trigger the Main Thread, UI-context when they
have saved any info. This architecture is often known as \emph{child-context},
because there is a parent-child relationship between all the
contexts~\ref{fig:coredataarch}. 

\subsubsection{Model design}
In our model we have three entities. One is the poster category, to separate
the different kind of posters that we can choose: movie posters, art, and so
on. Then, we have two entities for the posters. One is an \emph{abstract}
entity that the main Poster entity inherits from. 

This is a very common way to build models in CoreData. Entity inheritance
enables developers to perform changes easily in the data model. Every time we
have to ship a new version of the application, if the model has changed we need
to perform a migration. It is quite common to change the model when new
features are added, but migrations are difficult to implement if there are deep
changes. One way to make migrations easier is to use entity inheritance. If
somehow in the near future we need to add different posters, we can inherit
from the \texttt{AbstractPoster} entity without touching the \texttt{Poster}
one. 

With this model design we can take advantage of \emph{Lightweight migrations}
in CoreData. Lightweight migrations are automatic migrations performed
 by the framework. Being able to maintain our model versioning 
using this kind of migrations is another reason of why we are using entity
inheritance. Otherwise, we would have to perform manual migrations, which are
slower to compute and error prone.

\section{Augmented reality performance}
\label{sec:arperf}
During the development of Ponster, both OpenCV and Vuforia have been tested. Due to
the low performance of OpenCV on the mobile device, Vuforia has been the chosen library
to bring the augmented reality to our app. In the performance
chart~\ref{fig:performance} we can find the average time to analyze each frame with
the different technologies tested. 

All of these tests have been executed with the iPhone 5 mobile device using iOS
8. The selected pattern to test the matching is the famous image of Lena SÃ¶derberg,
which is widely used when testing computer vision algorithms. The test consists on
running the algorithm against the Lena image while moving and rotating the camera
for a minute.

\begin{figure}
\centering
\includegraphics[scale=0.75]{performance/plot.pdf}
\caption{\label{fig:performance} Average time spent processing each frame with the
  different algorithms. The best technique evaluated, Vuforia, is 10 times faster
  than SURF, which has the poorest performance of all the tracking algorithms
  tested.} 
\end{figure} 

As we can see, SURF is, by far, the most expensive technique tested on the device. This is the
expected behavior, as it's stated in the cited articles in
section~\ref{chap:sota}. FREAK uses SURF to compute 
the keypoints, so the performance boost happens on the descriptor calculation
only. Between all the OpenCV algorithms tested, ORB is the one that performs better
while providing almost the same robustness as the other two algorithms tested.
However, the only technique that led to a real-time performance has been the Vuforia
SDK. The average frames per second of the processing with Vuforia is between 29 and
30 FPS, while maintaining the best robustness and tracking of all the above
algorithms tested. This is due to the natural feature
approximation of Vuforia~\ref{sec:naturalfeature}.

The reason behind testing only SURF, FREAK and ORB under OpenCV is that they
represent different approaches to the same problem of object tracking. Analyzing
each part of the feature detection process can bring some light to know which step
is the most expensive in terms of computational cost. In the
figure~\ref{fig:kpperformance} we can see the average time spent in each frame when
computing the keypoints with SURF and ORB detectors. FREAK has been tested with SURF
keypoint detection, so we can clearly see that the descriptor extraction by FREAK is
much more efficient than the SURF extraction.

\begin{figure}
\centering
\includegraphics[scale=0.75]{performance/keypoints.pdf}
\caption{\label{fig:kpperformance} Mean time spent for detecting each frame
  keypoints with SURF and ORB.}
\end{figure} 

The number of keypoints detected between SURF and ORB is also quite different. In
the figure~\ref{fig:numberkp} we can see the average number of keypoint detected for
each frame. It is interesting to see that while ORB detects around 60\% of the
keypoints detected by SURF, it performs almost as good as SURF in terms of
robustness when detecting the selected pattern. However, while executing both
algorithms in the app, the detection of the pattern is less prone to error with
blurred or translated image using the SURF keypoint and descriptor combination. 

\begin{figure}
\centering
\includegraphics[scale=0.75]{performance/keypoints-number.pdf}
\caption{\label{fig:numberkp} Average number of keypoints detected using SURF and
  ORB keypoint detection algorithms.}
\end{figure} 

In the figure~\ref{fig:descriptors} we can see the difference of performance between
the descriptor extractors. The difference of performance between SURF keypoint
detector and descriptor extractor and SURF keypoint detector and FREAK descriptor
extractor can be explained because FREAK takes much less time to compute the
descriptors than SURF. This is an expected behavior, as it can be seen in Table 1
of the FREAK paper~\cite{Ortiz:2012:FFR:2354409.2354903}, where the computation time
for a descriptor using SIFT, SURF, BRISK and FREAK is compared. In that comparison,
FREAK is 77 times faster than SURF to compute descriptors.

\begin{figure}
\centering
\includegraphics[scale=0.75]{performance/descriptors.pdf}
\caption{\label{fig:descriptors} Average time to extract descriptors with SURF,
  FREAK and ORB.}
\end{figure} 

Finally, when we have extracted the descriptors of the image from the camera, we
have to match them with the descriptors obtained from the pattern image. Once the
match is performed, we have to calculate the homography to locate the object in the
image. Then, we can get the corners of the pattern and calculate the perspective
transform in order to draw the poster above the detected image. 

In the figure~\ref{fig:goodmatches} we can see that the SURF and Flann-based matcher
combination is the one that produces more matches, but it's also the more expensive
in terms of computational cost. SURF keypoint detector with FREAK descriptors
perform faster than the SURF approximation, but it also has lower precision when
detecting the pattern. Finally, ORB and Flann with Locality-Sensitive Hashing
parameters for the matching is the one that outperforms the previous techniques and
delivers an average number of matches similar to SURF.

\begin{figure}
\centering
\includegraphics[scale=0.75]{performance/goodmatches.pdf}
\caption{\label{fig:goodmatches} Average number of good matches detected using the
  different algorithm and matcher combinations.}
\end{figure} 

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{table}
\begin{tabular}{|l|p{2.05cm}|p{1.9cm}|p{2cm}|p{1.85cm}|p{1.9cm}|}
\hline\hline
\multicolumn{6}{|c|}{\large\bfseries Performance comparison between algorithms}
\\ \hline
\sffamily Algorithms & \sffamily Avg. Frame (s) & \sffamily Avg. Keypoint (s) &
\sffamily Avg. Descriptor (s) & \sffamily Avg. number
keypoints & \sffamily Avg. good matches \\ \hline
SURF & 0.3111817 & 0.1638936 & 0.260446452 & 845.4911 & 70.65957 \\ \hline
FREAK & 0.2044958 & --- & 0.005685782 & --- & 27.62121 \\ \hline
ORB & 0.08778909 & 0.02553993 & 0.015048248 & 481.1667 & 59.21101 \\ \hline
Vuforia & 0.03273424 & --- & --- & --- & --- \\ \hline\hline
%\captionof{figure}{Data used to build the performance figures.}
%\label{label}
\end{tabular}
\caption{Data used to build the performance figures.}
\end{table}
\end{center}

As we have stated before, Vuforia is a propietary SDK based on natural feature
detection, and we do not have access to how it detects the pattern. This is the
reason why it is only shown in the figure~\ref{fig:performance}, because we can
estimate the time spent processing for each frame delivered by the camera.

\section{Features}
The Ponster app has three main features: list posters, try how they look wherever
the user wants and capture an screen-shot of the poster in the scene. 

\subsection*{List posters}
The main screen of Ponster shows a complete list of the images available to
test. In the figure~\ref{fig:iosapp01} we can see how this screen is. Each of
the images represented come from a \texttt{Poster} entity from the
database. This posters are queried immediately using a special class called
\texttt{NS\-Fetched\-Results\-Controller}. 

This screen is ready for reading data from an API, because the way it has been
implemented enables to parse a JSON into a \texttt{Poster} entity and
automatically update the collection view of all the posters. 

The user can scroll the screen to see all the posters and when tapping on one,
a new view is presented with more information about the poster and the ability
to test the augmented reality using both the OpenCV approximation and the
Vuforia SDK, as it can be seen in figure~\ref{fig:posterview}.

\begin{figure}
\centering
\includegraphics[scale=0.55]{img/posterview.png}
\caption{\label{fig:posterview} The poster view of Ponster app.}
\end{figure} 

\subsection*{Augmented reality}
The main feature of Ponster is to show the posters where the user wants to try
them, using augmented reality techniques. Tapping on the \textbf{Try Me} button
opens the Vuforia window with the camera output. Once there, we present two
buttons, one for the Screen-shot~\ref{sub:screenshot} feature and another one
for the selection of the pattern to track. 

When we tap the camera button, we capture an image from the video output and
use it as our pattern to track. Then, every time the Vuforia SDK detects the
pattern, the selected poster is presented. The extended tracking is also
enabled, so the user can still see the poster even if the pattern is not
anymore present in the video output.

For testing purposes, we also bring the opportunity to test the augmented
reality using OpenCV algorithms. This feature has been used to run the speed
and robustness tests presented in the performance section~\ref{sec:arperf}.

\subsection*{Screen-shot}
\label{sub:screenshot}
The screen-shot feature enables the user to take a picture of what's seen on the
video output. This is very useful when it is combined with the augmented
reality tracking, and allows the user to keep a still picture in the camera
roll with the poster located in the scene. With this, the tested posters in the
scene can be shared.

\begin{figure}
\centering
\includegraphics[scale=0.45]{img/track01.JPG}
\caption{\label{fig:track01} Augmented reality using a frame as our marker to
  introduce the poster in the scene.}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.45]{img/track02.JPG}
\caption{\label{fig:track02} Another example of augmented reality using another
  pattern to track. In this case, defining a better located pattern results on
  having more realistic results. Note that the pattern selected is the white frame at the size
of the camera viewport, so thanks to Vuforia's robustness we are still able to track
the object despite the scale and perspective changes.}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.3]{img/track03.JPG}
\caption{\label{fig:track03} Thanks to the use of the gyroscope and accelerometer
  provided by CoreMotion framework, Vuforia manages to track correctly the pattern
  even if it is not entirely present in the scene.}
\end{figure} 

\begin{figure}
\centering
\includegraphics[scale=0.3]{img/track04.JPG}
\caption{\label{fig:track04} Despite the small size of the marker defined, we are
  still capable of detecting it and presenting the object correctly in the scene.}
\end{figure} 
