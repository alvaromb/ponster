\chapter{Technologies}
%Describe here Vuforia, OpenCV, FastCV, UIKit, etc.
Ponster is an augmented reality app developed for the iOS platform. In order to make
possible all the features that enable us to try the poster images in the camera
scene, two main technologies have been used. First of all, the iOS SDK provided by
Apple\textregistered ~is needed to develop any iOS application. For the augmented reality, OpenCV
has been the first SDK tested, but finally the Vuforia SDK\cite{vuforia} developed
by Qualcomm\textregistered ~has been the one used in Ponster. We will start this
discussing first the iOS SDK, and then the augmented reality technologies used.

\section{iOS}
The Apple iOS SDK has been used to develop the native, augmented reality
application. The development started using the 7.1 version and the final release has
been made with the latest SDK version, iOS 8. Several frameworks have been used to
enable us to show the posters with a waterfall layout, fetch the data of the posters
from the local database and to get input from the camera of the device.

The most important frameworks used are described in the next sections, and include
UIKit, CoreData and third party libraries.

\subsection{UIKit}
UIKit\cite{uikit} is Apple's framework to build iOS interfaces both in iPad and
iPhone devices. All of the UIKit's classes inherit from a common interface object
called \texttt{NSObject}. This framework provides classes to manage gestures, fonts,
navigation bars, tab bars, text inputs, images, tables, buttons and many more
elements. Almost every iOS application uses UIKit in one way or another. Only some
videogames do not make extensive use of the framework due to the specific user
interfaces and game engines that most of them are based on.

Most of the Apple's frameworks naming convention date back from the NeXTStep era. They're written in
Objective-C and they use the two-letter class prefix in all of them, making easy to
identify to which framework belongs each class. For instance, all the UIKit classes
have the \texttt{UI} class prefix, so \texttt{UITextField} or \texttt{UIView} both
belong to UIKit. In other hand, \texttt{NSObject}, which belongs to Foundation
framework, has the NeXTStep \texttt{NS} prefix.

In Ponster several UIKit features have been used. For the main user interface, a
navigation-based UI is provided by \texttt{UINavigationController}. The main screen
of Ponster shows a \texttt{UICollectionViewLayout} interface with a custom waterfall
layout. This layout enables us to fit images with different sizes preserving their
aspect ratio. Each image represents a subclass of \texttt{UICollectionViewCell} with
custom elements, such as a \texttt{UIImageView} for the poster and a \texttt{UILabel}
for it's title. In the image
\ref{fig:iosapp01} we can see an example of the navigation-based interface of
Ponster and the collection view layout.

\begin{figure}
\centering
\includegraphics[scale=0.35]{img/iosapp01.png}
\caption{\label{fig:iosapp01}Main interface of Ponster demonstrating the use of
  navigation controllers and collection layouts.}
\end{figure} 

Other UIKit features used in Ponster include \texttt{UIButton}s, \texttt{UISwitch}
and \texttt{UIGestureRecognizer}s. 

\subsection{CoreData, persistence layer for iOS applications}
The main purpose of the research presented in the chapter \ref{chap:sota} was to
bring augmented reality to display posters in any surface the user wants
to. However, Ponster app has been developed with the addition of more features in
mind. One of this features is data persistency.

In order to mantain a list of all the posters included in the app ---they are bundled
inside the app in this version---, we have several methods to include and display
this information. The simplest option would have been to save the data in the preferences
\texttt{.plist} file and then read all the values. Also, we could have used
\texttt{NSKeyedArchiver} to save data from our object model into the application
sandbox. But the best way to maintain data and to
query it from our application is to use a real SQL database. CoreData integrates
the SQLite database with a class-based model approach that enables us to save and
retrieve information easily.

With the CoreData framework, we design an object model (figure~\ref{fig:coredata}) and then we
generate the model classes. Each class is a subclass of \texttt{NSManagedObject}
that has all the attributes that we've added in our model. Inside the application,
we can query our model by class, and then retrieve the objects as an array. Each
model object is like any other object, and we can access to it's properties using
the common dot notation. For example, we can access to the \texttt{imageUrl}
property of a \texttt{Poster} entity in the following way:

\begin{verbatim}
Poster *poster = (Poster *)item;
UIImage *posterImage = [UIImage imageNamed:poster.imageUrl];
\end{verbatim}

\begin{figure}
\centering
\includegraphics[scale=0.65]{img/coredata.png}
\caption{\label{fig:coredata}Basic data model of Ponster. The CoreData model editor
  enables us to design the model and then generate all the object classes. The
  double arrow represents a to-many relationship and the white arrow represents
  object inheritance.}
\end{figure} 

CoreData's stack enables us to perform queries and to save information in the SQLite
store. Due to the fact that iOS applications use a maximum-priority thread for the
UI calculations and several other threads with lower priority to perform other
tasks, it's fundamental to understand how CoreData performs it's operations and how
can we use it efficiently. In the section~\ref{sec:architecture}, our CoreData
architecture is presented.

Three objects are fundamental to CoreData, the \textbf{context}, the \textbf{coordinator}
and the \textbf{store}. The \texttt{NSManagedObject\textbf{Context}} is used to perform
any save or query operation to the CoreData stack. The
\texttt{NSPersistentStore\textbf{Coordinator}} is the object that communicates
between the contexts and the stores. Finally, the
\texttt{NSPersistent\textbf{Store}} is the responsable of applying the changes to
the SQLite backend. The store and the coordinator are initialized once you start
your app. All the data that we've saved is present in the SQLite file, inside our
application sandbox. Apps can use more than one context, as we discuss
in~\ref{sec:architecture}, and every operation against the database must be
performed in a context. For example, when we want to perform a request for a
selected \textit{entity}, we have to specify the context:

\begin{verbatim}
NSFetchRequest *request = [[NSFetchRequest alloc] init];
NSEntityDescription *entityDescription = [NSEntityDescription
entityForName:entityName inManagedObjectContext:context];
[request setEntity:entityDescription];
// Build and perform the query
[request setPredicate:...];
\end{verbatim}

Finally, CoreData works very well with another important class,
\texttt{NSFetchedResultsController}. This class allows us to watch for changes in
any class in our model, with any predicate we want, and then perform an automatic
refresh of our data every time some change in our model has been done in the
observed classes. Thus, the fetched results controller is the class that comunicates
our view controllers with the data model.

\subsection{AVFoundation}
AVFoundation is used in Ponster to get the images from the camera of the device. A
\texttt{AVCaptureVideoDataOutput} can be configured to get the frames of the
camera. This output can be configured to deliver an specific amount of frames per
second, the pixel format of the output or the orientation of the camera. Each of the
frames delivered by the \texttt{AVCaptureVideoDataOutputSampleBufferDelegate} is
sent to the augmented reality algorithm to perform the matching.

\subsection{Third-party libraries}
Several third-party libraries are used in Ponster. One of them is the OpenCV SDK and
Vuforia SDK, which we're going to discuss in the next section. The other two
interesting libraries are \textbf{MagicalRecord} and
\textbf{PDKTCollectionViewWaterfallLayout}, along with the package dependency
manager \textbf{CocoaPods}.

\begin{description}
\item [MagicalRecord] \hfill \\
MagicalRecord\cite{gh:mr} is a library written by Saul Mora that makes easy to use
CoreData. It acts as a wrapper for most of the CoreData's actions and makes possible
to perform any action with less lines of code. Also, MagicalRecord helps us to create new
contexts, which is very important in our application architecture.
\item [PDKTCollectionViewWaterfallLayout] \hfill \\
PDKTCollectionViewWaterfallLayout is a \texttt{UICollectionViewLayout} that enables
us to organize the cells of the collection view in a waterfall-style layout. With
this we can display different cell sizes by preserving their aspect ratio and
maintaining the same width.
\end{description}

\subsubsection{CocoaPods dependency manager}
This two third-party libraries, along with OpenCV, have been installed and added to
Ponster by using CocoaPods. CocoaPods is the dependency manager for Objective-C
projects\cite{cocoapods}. It is an open-source project sponsored by several
companies that it's becoming very popular among iOS and Mac developers.

With CocoaPods it is easy for developers to generate a \emph{Podspec} for their
projects and share them with the community. For instance, an iOS OpenCV compilation
is maintained by several developers and they have recently added a Podspec ---a small
file with the description of the version of the project, the repository of the
source code and their dependencies--- to make easier to integrate OpenCV in iOS
projects. With CocoaPods we just have to create a \texttt{Podfile} with all the
dependencies we want to install, and then by just entering \texttt{pod install} in
the project directory, all the dependencies are installed and linked as a
subproject.

This dependency manager allows developers to easily add or remove dependencies to
their projects, and also makes easy to share their libraries with others. CocoaPods
is distributed as a Ruby gem and it can be installed easily in any Mac computer.

\section{Computer vision}
Two computer vision libraries have been used, OpenCV in the first stages of
development, and later Vuforia, which is made by Qualcomm\textregistered. 

\subsection{OpenCV}
OpenCV is an open source computer vision library\cite{opencv}. The library has
more than 2,500 computer vision and machine learning algorithms. It has been
released as a BSD-licensed source, so most of the code it contains can be used for
commercial and non-commercial applications.

The OpenCV library can be used in Windows, GNU/Linux and OS X platforms. The
latest OpenCV library available for iOS platforms is the 2.4.9
build\cite{opencvpod}. Some of the algorithms used in Ponster --like SURF or SIFT--
are patented, so they are available in the \texttt{nonfree} headers. These
algorithms cannot be used in commercial applications, but an implementatios is
included in the library with educational purposes. 

There are a lot of applications that use OpenCV. C++ and Python programming
languages are supported, among others. The C++ version is the one that has been
tested in Ponster, using C++ and Objective-C++ files. Although OpenCV has a
very good performance in most platforms, the Vuforia alternative performed much
faster and with a constant 30 fps frame rate.

\subsection{Vuforia}
Vuforia is an augmented reality SDK developed by the American chip company
Qualcomm\textregistered. The Vuforia API supports C++, Objective-C, Java and .NET
---with the Unity game engine---. Currently, Vuforia is at it's 3.0 version. 

The SDK consists of several core components\cite{vuforiasdk01}: 
\begin{description}
\item [Camera] \hfill \\
Delivers the image frame with a custom data structure, independent from the
platform.
\item [Image Converter] \hfill \\
This downsamples the image for a better performance and converts the image format to
a suitable OpenGL ES format.
\item [Tracker] \hfill \\
The tracker performs the computer vision algorithms and stores the result in a
shared object used by the background renderer. The tracker has many different
detection algorithms that can be used depending on the camera image.
\item [Video Background Renderer] \hfill \\
This is the responsible of rendering the video image into the screen. It's optimized
for the different platforms that Vuforia works with.
\item [Application Code] \hfill \\
For each processed frame, we have to perform three main steps: query the state
object for newly detected targets or updated states, update the application with the
new information received and render the graphics. This scheme is almost identical to
the first custom scheme used when testing OpenCV feature detection methods.
\item [User-defined Targets] \hfill \\
In Ponster, users can define a tracking object by taking a picture of the scene,
thus enabling to use any image as a placeholder of the poster. This works by
processing the captured frame and caching it's result for the augmented reality
session. Vuforia enables developers to use pre-defined targets also.
\end{description}

\begin{figure}
\centering
\includegraphics[scale=0.45]{img/vuforiasdk.png}
\caption{\label{fig:vuforiasdk}Vuforia SDK architecture.}
\end{figure} 

One of the main advantages of Vuforia SDK is that it is optimized to run in
ARM-based devices. In the tests performed during the development of Ponster, Vuforia
has shown a clear advantage in terms of frames per second processed in comparison
with OpenCV. While SURF performed roughly at 1 or 2 fps, Vuforia has a constant fps
rate of 30 to 29 frames.

\subsubsection{Extended tracking}
Although we don't know exactly how Vuforia tracking algorithms internally work, some
parts of the API architecture are explained in the Vuforia developer website. In the
chapter~\ref{chap:sota} we have explained how the tracking algorithm uses natural
feature detection. It is worth to mention how the extended tracking works and what
it represents when the features detected are not good enough because it does not
appear entirely on the screen.

The extended tracking enables augmented reality applications to maintain the object
detection even if the whole pattern is not visible. With iOS, Vuforia can bring this
using CoreMotion framework. It is not defined how exactly Vuforia makes extended
tracking possible under iOS applications, but it is sure that they use the
accelerometer and the gyroscope to keep the object visible in the scene while the
user moves the smartphone camera changing the scene.

\subsubsection{FastCV}
Vuforia is based on FastCV, which is another library developed by
Qualcomm\textregistered that brings computer vision algorithms optimized for mobile
architectures. FastCV has the main computer vision features\cite{fastcv}:

\begin{itemize}
\item Gesture recognition algorithms.
\item Face detection, tracking and pattern recognition.
\item Augmented reality.
\end{itemize}

When running on ARM devices, FastCV is CPU-optimized. Due to the fact that Qualcomm
builds the Snapdragon ARM microchip, FastCV is finely tuned for that artchitecture. 
Internally, Vuforia uses all of the FastCV algorithms, but it also takes advantage
of other inputs such as the gyroscope or the accelerometer.
